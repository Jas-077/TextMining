{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1h8VvImrvX4Y"
      },
      "outputs": [],
      "source": [
        "import google.colab.drive as drive\n",
        "import pandas as pd\n",
        "import copy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cjslGC_kvgFT",
        "outputId": "de7aa4fa-11ad-426a-8cfe-bbbcb56edfb8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5zMV_Gf7xOo8"
      },
      "outputs": [],
      "source": [
        "data = pd.read_csv('/content/drive/MyDrive/Text Mining/pii-detection-removal-from-educational-data/labeled_data.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kFPkH45C6TWY"
      },
      "outputs": [],
      "source": [
        "data['Text'] = copy.deepcopy(data['tweet'])\n",
        "data['Label'] = copy.deepcopy(data['class'])"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data['Label']"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uAKzGYT28J5c",
        "outputId": "382e1e4e-85ad-4927-9b27-1fbe32d2e2ca"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0        2\n",
              "1        1\n",
              "2        1\n",
              "3        1\n",
              "4        1\n",
              "        ..\n",
              "24778    1\n",
              "24779    2\n",
              "24780    1\n",
              "24781    1\n",
              "24782    2\n",
              "Name: Label, Length: 24783, dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data['Text']"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5kMOZrpWMF1a",
        "outputId": "8557806f-a5a1-4418-d9ae-fc3d7aeceb5d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0        !!! RT @mayasolovely: As a woman you shouldn't...\n",
              "1        !!!!! RT @mleew17: boy dats cold...tyga dwn ba...\n",
              "2        !!!!!!! RT @UrKindOfBrand Dawg!!!! RT @80sbaby...\n",
              "3        !!!!!!!!! RT @C_G_Anderson: @viva_based she lo...\n",
              "4        !!!!!!!!!!!!! RT @ShenikaRoberts: The shit you...\n",
              "                               ...                        \n",
              "24778    you's a muthaf***in lie &#8220;@LifeAsKing: @2...\n",
              "24779    you've gone and broke the wrong heart baby, an...\n",
              "24780    young buck wanna eat!!.. dat nigguh like I ain...\n",
              "24781                youu got wild bitches tellin you lies\n",
              "24782    ~~Ruffled | Ntac Eileen Dahlia - Beautiful col...\n",
              "Name: Text, Length: 24783, dtype: object"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U160y7tgAqNv"
      },
      "outputs": [],
      "source": [
        "from transformers import XLNetTokenizer, XLNetForSequenceClassification, AdamW\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "import string\n",
        "import copy\n",
        "from transformers import RobertaTokenizer, RobertaForSequenceClassification, AdamW\n",
        "from transformers import DistilBertTokenizer, DistilBertForSequenceClassification, AdamW\n",
        "\n",
        "\n",
        "\n",
        "# Load pre-trained XLNet tokenizer\n",
        "tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n",
        "\n",
        "# Assume data is your DataFrame with 'Text' and 'Label' columns\n",
        "texts = data['Text'].tolist()  # Your list of text samples\n",
        "labels = data['Label'].tolist()  # Corresponding labels for each text sample\n",
        "\n",
        "# Encode labels\n",
        "le = LabelEncoder()\n",
        "labels_encoded = le.fit_transform(labels)\n",
        "num_labels = len(le.classes_)\n",
        "# Split the data into train and validation sets\n",
        "train_texts, val_texts, train_labels, val_labels = train_test_split(texts, labels_encoded, test_size=0.2, random_state=85)\n",
        "\n",
        "# Tokenize and encode the text data\n",
        "train_encodings = tokenizer(train_texts, truncation=True, padding=True)\n",
        "val_encodings = tokenizer(val_texts, truncation=True, padding=True)\n",
        "\n",
        "# Convert the tokenized data to PyTorch tensors\n",
        "train_dataset = torch.utils.data.TensorDataset(\n",
        "    torch.tensor(train_encodings['input_ids']),\n",
        "    torch.tensor(train_encodings['attention_mask']),\n",
        "    torch.tensor(train_labels)\n",
        ")\n",
        "val_dataset = torch.utils.data.TensorDataset(\n",
        "    torch.tensor(val_encodings['input_ids']),\n",
        "    torch.tensor(val_encodings['attention_mask']),\n",
        "    torch.tensor(val_labels)\n",
        ")\n",
        "\n",
        "# Load pre-trained XLNet model for sequence classification\n",
        "model = DistilBertForSequenceClassification.from_pretrained('distilbert-base-uncased', num_labels=num_labels)\n",
        "\n",
        "# Define optimizer and learning rate scheduler\n",
        "optimizer = AdamW(model.parameters(), lr=1e-5)\n",
        "\n",
        "# Define data loaders\n",
        "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=8, shuffle=True)\n",
        "val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=8)\n",
        "\n",
        "# Training loop\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "# device = torch.device('cpu')\n",
        "model.to(device)\n",
        "\n",
        "best_model_state_dict = None\n",
        "best_val_accuracy = 0  # Track best validation accuracy\n",
        "best_val_loss = np.inf  # Track best validation loss\n",
        "early_stopping_patience = 3  # Number of epochs to wait for early stopping\n",
        "early_stopping_counter = 0\n",
        "training_losses = []\n",
        "validation_losses = []\n",
        "training_accuracies = []\n",
        "validation_accuracies = []\n",
        "\n",
        "for epoch in range(10):  # Change the number of epochs as needed\n",
        "    model.train()\n",
        "    train_losses = []\n",
        "    train_preds = []\n",
        "    train_true = []\n",
        "\n",
        "    for batch in train_loader:\n",
        "        input_ids, attention_mask, labels = batch\n",
        "        input_ids, attention_mask, labels = input_ids.to(device), attention_mask.to(device), labels.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
        "        loss = outputs.loss\n",
        "        train_losses.append(loss.item())\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        preds = torch.argmax(outputs.logits, axis=1)\n",
        "        train_preds.extend(preds.cpu().numpy())\n",
        "        train_true.extend(labels.cpu().numpy())\n",
        "\n",
        "    train_accuracy = accuracy_score(train_true, train_preds)\n",
        "    avg_train_loss = np.mean(train_losses)\n",
        "\n",
        "    # Validation\n",
        "    model.eval()\n",
        "    val_preds = []\n",
        "    val_true = []\n",
        "    val_losses = []\n",
        "\n",
        "    for batch in val_loader:\n",
        "        input_ids, attention_mask, labels = batch\n",
        "        input_ids, attention_mask, labels = input_ids.to(device), attention_mask.to(device), labels.to(device)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            outputs = model(input_ids, attention_mask=attention_mask)\n",
        "\n",
        "        logits = outputs.logits\n",
        "        loss = F.cross_entropy(logits, labels)\n",
        "        val_losses.append(loss.item())\n",
        "\n",
        "        preds = torch.argmax(logits, axis=1)\n",
        "        val_preds.extend(preds.cpu().numpy())\n",
        "        val_true.extend(labels.cpu().numpy())\n",
        "\n",
        "    val_accuracy = accuracy_score(val_true, val_preds)\n",
        "    avg_val_loss = np.mean(val_losses)\n",
        "\n",
        "    print(f'Epoch {epoch + 1}: Training Accuracy: {train_accuracy:.4f}, Training Loss: {avg_train_loss:.4f}, Validation Accuracy: {val_accuracy:.4f}, Validation Loss: {avg_val_loss:.4f}')\n",
        "    training_losses.append(avg_train_loss)\n",
        "    validation_losses.append(avg_val_loss)\n",
        "    training_accuracies.append(train_accuracy)\n",
        "    validation_accuracies.append(val_accuracy)\n",
        "    # Update best model\n",
        "    if avg_val_loss < best_val_loss:\n",
        "        best_val_loss = avg_val_loss\n",
        "        early_stopping_counter = 0\n",
        "    else:\n",
        "        early_stopping_counter += 1\n",
        "\n",
        "    if val_accuracy > best_val_accuracy:\n",
        "        best_val_accuracy = val_accuracy\n",
        "        best_model_state_dict = copy.deepcopy(model.state_dict())\n",
        "        print('New Best Model')\n",
        "\n",
        "    if early_stopping_counter >= early_stopping_patience:\n",
        "        print('Early stopping triggered.')\n",
        "        break\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zm7VmigqlCS_",
        "outputId": "2a3020ea-d1a2-4520-c935-4e5876386f9c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.56      0.23      0.32       273\n",
            "           1       0.94      0.97      0.95      3867\n",
            "           2       0.87      0.90      0.88       817\n",
            "\n",
            "    accuracy                           0.92      4957\n",
            "   macro avg       0.79      0.70      0.72      4957\n",
            "weighted avg       0.90      0.92      0.91      4957\n",
            "\n"
          ]
        }
      ],
      "source": [
        "model.load_state_dict(best_model_state_dict)\n",
        "model.eval()\n",
        "model.to(device)\n",
        "# Evaluate on validation set\n",
        "val_preds = []\n",
        "val_true = []\n",
        "for batch in val_loader:\n",
        "    input_ids, attention_mask, labels = batch\n",
        "    input_ids, attention_mask, labels = input_ids.to(device), attention_mask.to(device), labels.to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        outputs = model(input_ids, attention_mask=attention_mask)\n",
        "\n",
        "    logits = outputs.logits\n",
        "    preds = torch.argmax(logits, axis=1)\n",
        "    val_preds.extend(preds.cpu().numpy())\n",
        "    val_true.extend(labels.cpu().numpy())\n",
        "val_preds = le.inverse_transform(val_preds)\n",
        "val_true = le.inverse_transform(val_true)\n",
        "# Print classification report\n",
        "print(classification_report(val_true, val_preds))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Training with class weights of [5.0, 1.0, 1.0]**"
      ],
      "metadata": {
        "id": "9UJbUzC7ah1r"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1Pl4uxbnDen5"
      },
      "outputs": [],
      "source": [
        "from transformers import XLNetTokenizer, XLNetForSequenceClassification, AdamW\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "import string\n",
        "import copy\n",
        "from transformers import RobertaTokenizer, RobertaForSequenceClassification, AdamW\n",
        "from transformers import DistilBertTokenizer, DistilBertForSequenceClassification, AdamW\n",
        "\n",
        "\n",
        "\n",
        "# Load pre-trained XLNet tokenizer\n",
        "tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n",
        " # Adjust weights as needed for your classes\n",
        "\n",
        "# Assume data is your DataFrame with 'Text' and 'Label' columns\n",
        "texts = data['Text'].tolist()  # Your list of text samples\n",
        "labels = data['Label'].tolist()  # Corresponding labels for each text sample\n",
        "\n",
        "# Encode labels\n",
        "le = LabelEncoder()\n",
        "labels_encoded = le.fit_transform(labels)\n",
        "num_labels = len(le.classes_)\n",
        "# Split the data into train and validation sets\n",
        "train_texts, val_texts, train_labels, val_labels = train_test_split(texts, labels_encoded, test_size=0.2, random_state=85)\n",
        "\n",
        "# Tokenize and encode the text data\n",
        "train_encodings = tokenizer(train_texts, truncation=True, padding=True)\n",
        "val_encodings = tokenizer(val_texts, truncation=True, padding=True)\n",
        "\n",
        "# Convert the tokenized data to PyTorch tensors\n",
        "train_dataset = torch.utils.data.TensorDataset(\n",
        "    torch.tensor(train_encodings['input_ids']),\n",
        "    torch.tensor(train_encodings['attention_mask']),\n",
        "    torch.tensor(train_labels)\n",
        ")\n",
        "val_dataset = torch.utils.data.TensorDataset(\n",
        "    torch.tensor(val_encodings['input_ids']),\n",
        "    torch.tensor(val_encodings['attention_mask']),\n",
        "    torch.tensor(val_labels)\n",
        ")\n",
        "\n",
        "# Load pre-trained XLNet model for sequence classification\n",
        "model = DistilBertForSequenceClassification.from_pretrained('distilbert-base-uncased', num_labels=num_labels)\n",
        "\n",
        "# Define optimizer and learning rate scheduler\n",
        "optimizer = AdamW(model.parameters(), lr=1e-5)\n",
        "\n",
        "# Define data loaders\n",
        "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=8, shuffle=True)\n",
        "val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=8)\n",
        "\n",
        "# Training loop\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "class_weights = torch.tensor([5.0, 1.0, 1.0]).to(device)\n",
        "# device = torch.device('cpu')\n",
        "model.to(device)\n",
        "\n",
        "best_model_state_dict = None\n",
        "best_val_accuracy = 0  # Track best validation accuracy\n",
        "best_val_loss = np.inf  # Track best validation loss\n",
        "early_stopping_patience = 3  # Number of epochs to wait for early stopping\n",
        "early_stopping_counter = 0\n",
        "training_losses = []\n",
        "validation_losses = []\n",
        "training_accuracies = []\n",
        "validation_accuracies = []\n",
        "\n",
        "for epoch in range(10):  # Change the number of epochs as needed\n",
        "    model.train()\n",
        "    train_losses = []\n",
        "    train_preds = []\n",
        "    train_true = []\n",
        "\n",
        "    for batch in train_loader:\n",
        "        input_ids, attention_mask, labels = batch\n",
        "        input_ids, attention_mask, labels = input_ids.to(device), attention_mask.to(device), labels.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
        "        loss = F.cross_entropy(outputs.logits, labels, weight=class_weights)\n",
        "        train_losses.append(loss.item())\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        preds = torch.argmax(outputs.logits, axis=1)\n",
        "        train_preds.extend(preds.cpu().numpy())\n",
        "        train_true.extend(labels.cpu().numpy())\n",
        "\n",
        "    train_accuracy = accuracy_score(train_true, train_preds)\n",
        "    avg_train_loss = np.mean(train_losses)\n",
        "\n",
        "    # Validation\n",
        "    model.eval()\n",
        "    val_preds = []\n",
        "    val_true = []\n",
        "    val_losses = []\n",
        "\n",
        "    for batch in val_loader:\n",
        "        input_ids, attention_mask, labels = batch\n",
        "        input_ids, attention_mask, labels = input_ids.to(device), attention_mask.to(device), labels.to(device)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            outputs = model(input_ids, attention_mask=attention_mask)\n",
        "\n",
        "        logits = outputs.logits\n",
        "        loss = F.cross_entropy(logits, labels)\n",
        "        val_losses.append(loss.item())\n",
        "\n",
        "        preds = torch.argmax(logits, axis=1)\n",
        "        val_preds.extend(preds.cpu().numpy())\n",
        "        val_true.extend(labels.cpu().numpy())\n",
        "\n",
        "    val_accuracy = accuracy_score(val_true, val_preds)\n",
        "    avg_val_loss = np.mean(val_losses)\n",
        "\n",
        "    print(f'Epoch {epoch + 1}: Training Accuracy: {train_accuracy:.4f}, Training Loss: {avg_train_loss:.4f}, Validation Accuracy: {val_accuracy:.4f}, Validation Loss: {avg_val_loss:.4f}')\n",
        "    training_losses.append(avg_train_loss)\n",
        "    validation_losses.append(avg_val_loss)\n",
        "    training_accuracies.append(train_accuracy)\n",
        "    validation_accuracies.append(val_accuracy)\n",
        "    # Update best model\n",
        "    if avg_val_loss < best_val_loss:\n",
        "        best_val_loss = avg_val_loss\n",
        "        early_stopping_counter = 0\n",
        "    else:\n",
        "        early_stopping_counter += 1\n",
        "\n",
        "    if val_accuracy > best_val_accuracy:\n",
        "        best_val_accuracy = val_accuracy\n",
        "        best_model_state_dict = copy.deepcopy(model.state_dict())\n",
        "        print('New Best Model')\n",
        "\n",
        "    if early_stopping_counter >= early_stopping_patience:\n",
        "        print('Early stopping triggered.')\n",
        "        break\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4a2f0793-219b-4033-c813-641c49a2e1c8",
        "id": "9SwFrCw_Den6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.44      0.54      0.49       273\n",
            "           1       0.95      0.94      0.95      3867\n",
            "           2       0.89      0.88      0.89       817\n",
            "\n",
            "    accuracy                           0.91      4957\n",
            "   macro avg       0.76      0.79      0.77      4957\n",
            "weighted avg       0.92      0.91      0.91      4957\n",
            "\n"
          ]
        }
      ],
      "source": [
        "model.load_state_dict(best_model_state_dict)\n",
        "model.eval()\n",
        "model.to(device)\n",
        "# Evaluate on validation set\n",
        "val_preds = []\n",
        "val_true = []\n",
        "for batch in val_loader:\n",
        "    input_ids, attention_mask, labels = batch\n",
        "    input_ids, attention_mask, labels = input_ids.to(device), attention_mask.to(device), labels.to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        outputs = model(input_ids, attention_mask=attention_mask)\n",
        "\n",
        "    logits = outputs.logits\n",
        "    preds = torch.argmax(logits, axis=1)\n",
        "    val_preds.extend(preds.cpu().numpy())\n",
        "    val_true.extend(labels.cpu().numpy())\n",
        "val_preds = le.inverse_transform(val_preds)\n",
        "val_true = le.inverse_transform(val_true)\n",
        "# Print classification report\n",
        "print(classification_report(val_true, val_preds))"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Puj1ZOiYYSpI"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}